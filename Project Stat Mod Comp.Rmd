---
title: "Project : Statistical Models and Computing"
author: "Aditya Shah"
output: html_document
date: "2025-04-02"
---

# Aditya Shah - RUID: 237004776 

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(MASS)
library(boot)
library(leaps)
library(pacman)
library(faraway)
library(datasets)
library(readxl)
library(pls)
library(glmnet)
library(ggplot2)
library(dplyr)
library(readr)
library(kernlab)
library(pROC)
library(ISLR2)
library(tidyverse)
library(MatchIt)
library(cobalt)
library(broom)
library(ggpubr)
library(sandwich)
library(lmtest)
library(arm)

```


## Project description:

You will conduct a data analysis to investigate the causal effect of a “nudge-like” intervention  geared towards instilling students with a growth mindset. Loosely speaking, a growth mindset is the belief that people can develop intelligence. This is in contrast with the belief that intelligence is acquired at birth and cannot be substantially improved; this is known as the fixed mindset. The dataset provided is synthetic, i.e., it has been generated by the instructors of this course. 

However, it is based on the true data from the actual National Study of Learning Mindsets 
experiment (NSLM) (Yeager et al, 2019). The key “nudge-like” intervention considered in the 
NSLM experiment consists of showing a randomly selected sample of units an online program 
about “people’s ability to grow their intellectual abilities over time and how this can help them make a difference on things that matter to them personally.” 
(https://studentexperiencenetwork.org/national-mindset-study/) 

The original experiment randomly assigned 10,000 students to the treatment and control from 76 schools sampled from U.S. public high schools. The synthetic dataset should be considered an observational study. In the dataset, you will find four student-level covariates and six school level covariates. These covariates are: 

- selfrpt: Student’s self-reported expectations for success in the future, a proxy for prior 
achievement, measured prior to treatment; 
- race: student race/ethnicity (categorical); 
- gender: student’s identified gender (categorical); 
- fgen: indicator for whether student is the first in family to go to college (first-generation, binary).  
- urban: school-level urbanicity of the school (categorical); 
- mindset: school-level mean of students’ fixed mindsets; reported prior to the observed treatment; 
- test: school-level achievement level, as measured by test scores and college preparation 
for the previous four cohorts of students; 
- sch_race: school-level racial/ethnic minority composition; 
- pov: school-level poverty concentration as measured by the percentage-age of students who are from families whose incomes fall below the federal poverty line; 
- size: total number of students in all four grade levels in the school. 

In addition to these ten covariates (plus the school ID), the outcome Y and treatment Z have been generated. The outcome is a generic, continuous measure of the student’s achievement. Again, you should consider the synthetic data to be observational not an experiment. 

Your goal is to estimate the average effect of the nudge-like intervention on achievement (i.e., the effect of Z on Y). When planning your analysis, you should focus on showcasing what you have learned in this course (e.g., causal inference, regression estimation, sound statistical thinking, etc.). Although focus should be on methods learned in class, you are free to also consider other estimation methods not covered in class, as long as they are relevant and accurately referenced and described. Furthermore, it is important to clearly state the assumptions needed for the statistical analysis to have a valid causal interpretation.

## Introduction and Data Exploration:

```{r}

mindset_data <- read_csv("C:/All Data/Studies/Msc Rutgers/Semester 2/Statistical Models and Computing/Project/data.csv")

```

```{r}

head(mindset_data)

```

```{r}

summary(mindset_data)

```

```{r}

glimpse(mindset_data)

```

## Exploratory Data Analysis (EDA)


Check for missing values

```{r}

colSums(is.na(mindset_data))

```

We have no missing values, so we don't need to clean anything there.

```{r}

table(mindset_data$z)

```

```{r}


ggplot(mindset_data, aes(x = as.factor(z), y = y, fill = as.factor(z))) +
  geom_boxplot() +
  labs(x = "Treatment (Z)", y = "Student Achievement (Y)", title = "Effect of Nudge-Like Intervention on Achievement") +
  theme_minimal()

```

The treatment group shows more variability, with a wider interquartile range (IQR) and more extreme values (outliers).

The control group has a more compact distribution around the median, with fewer extreme outliers.

The median student achievement appears higher in the treatment group (Z=1) than in the control group (Z=0), suggesting a potential positive effect of the intervention.


```{r}


ggplot(mindset_data, aes(x = y, fill = as.factor(z))) +
  geom_density(alpha = 0.5) +
  labs(title = "Density Plot of Achievement (Y) by Treatment", 
       x = "Student Achievement (Y)", 
       y = "Density", 
       fill = "Treatment (Z)") +
  theme_minimal()


```

The red distribution (control group, Z=0) is more concentrated around 0, with a peak at a slightly negative value.

The blue distribution (treatment group, Z=1) is shifted slightly to the right, suggesting that the intervention may have led to higher achievement scores.

The treatment group exhibits a longer right tail, meaning some students benefited more significantly from the intervention.

The control group has a higher peak, meaning their scores are more tightly concentrated around the central value.

```{r}

ggplot(mindset_data, aes(x = as.factor(z), y = y, fill = as.factor(z))) +
  geom_violin(trim = FALSE, alpha = 0.5) +
  geom_boxplot(width = 0.1, fill = "white") + 
  labs(title = "Violin Plot of Achievement (Y) by Treatment",
       x = "Treatment (Z)", 
       y = "Student Achievement (Y)") +
  theme_minimal()

```

The intervention appears to increase variability in achievement scores.

While the median is higher for the treated group, the overlap between groups suggests other factors might influence achievement.

```{r}

mindset_data %>%
  group_by(z) %>%
  summarise(across(c(selfrpt, mindset, test, pov, size), mean, na.rm = TRUE))

```

Treated students have higher self-reported expectations for future success, indicating possible selection into treatment.

Schools in the treatment group show a slightly more fixed mindset (lower growth mindset), suggesting potential differences in school environment.

Treated students attend schools with slightly higher average achievement scores.

Poverty levels are somewhat similar between groups, but slightly lower in the treated group.

Treated students are from slightly larger schools on average.

## Simple linear regression to estimate the treatment effect: 

```{r}

ols_model <- lm(y ~ z + selfrpt + race + gender + fgen + urban + mindset + test + sch_race + pov + size, data = mindset_data)

coeftest(ols_model, vcov = vcovHC(ols_model, type = "HC3"))

tidy(ols_model) %>%
  filter(term == "z") %>%
  ggplot(aes(x = term, y = estimate)) +
  geom_point(size = 4, color = "blue") +
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error), width = 0.2) +
  labs(title = "OLS Estimate of Treatment Effect", y = "Estimated Effect Size", x = "Treatment")



```

From the summary we can say that race is the only covariate that isn't statistically significant in predicting achievement (p = 0.2941), indicating minimal or no predictive value.

Strong evidence of a significant positive causal effect of the growth mindset intervention on student achievement (~0.41 units increase).

After adjusting for all covariates, the intervention maintains a robustly positive and statistically significant effect.

The narrow confidence interval and high statistical significance enhance the reliability and validity of your causal inference.

```{r}

mindset_data$predicted <- predict(ols_model)
mindset_data$residuals <- residuals(ols_model)

ggplot(mindset_data, aes(x = predicted, y = residuals)) +
  geom_jitter(width = 0.2, alpha = 0.2, color = "black") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Predicted Values (with Jitter)",
       x = "Predicted Values",
       y = "Residuals") +
  theme_minimal()

```

No clear pattern: Suggests a good fit of the linear model.

Random scatter around zero line: Supports assumptions of linearity and equal variance

## Bootstrap Inference: 

```{r}

bootstrap_estimate <- function(data, indices) {
  d <- data[indices, ]
  model <- lm(y ~ z + selfrpt + gender + fgen + urban + mindset + test + sch_race + pov + size, data = d)
  return(coef(model)["z"])
}

set.seed(954567)
boot_results <- boot(mindset_data, bootstrap_estimate, R = 1000)

boot_ci <- boot.ci(boot_results, type = "perc")
cat("Bootstrap confidence intervals : \n")
boot_ci

plot(boot_results)

```

The interval (0.3866,  0.4423) does not include zero, meaning there's strong statistical evidence that the growth mindset intervention has a significant positive effect on student achievement.

We are 95% confident that the true treatment effect of this intervention lies between approximately 0.3866 and 0.4423, indicating a robust and stable positive impact.

Since the entire confidence interval is relatively narrow and consistently positive, the estimate of the intervention’s effectiveness is precise and reliable.

This supports strong causal inference claims: the intervention effectively and consistently enhances student achievement.

The intervention’s estimated impact (around 0.4) is meaningful, confirming the robustness of your previous regression.

## Model Selection and Regularization:

```{r}

x <- model.matrix(y ~ z + selfrpt + gender + fgen + mindset + test + pov + size, data = mindset_data)[,-1]
y <- mindset_data$y

set.seed(954567)
cv_lasso <- cv.glmnet(x, y, alpha = 1)
plot(cv_lasso)

```

The plot shows mean squared error (MSE) on the y-axis and $log(\lambda)$ on the x-axis.

Each red dot represents a cross-validation result for a particular lambda value.

The left dotted vertical line indicates lambda.min, the \lambda with the lowest MSE.

The right dotted vertical line indicates lambda.1se, a more regularized model within one standard error of the minimum

```{r}

best_lambda <- cv_lasso$lambda.min

lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(lasso_model)

```

Treatment effect (Z) remains strong and positive, even after regularization, indicating robustness.

LASSO automatically eliminates non-contributing variables, like pov in this case.

The selected covariates are predictive and interpretable, contributing to a compact, effective model.

This improves generalization (avoids overfitting) while preserving explanatory power.

```{r}

lasso_coefs <- coef(lasso_model)
lasso_df <- as.data.frame(as.matrix(lasso_coefs))
lasso_df <- tibble::rownames_to_column(lasso_df, var = "Variable")
colnames(lasso_df)[2] <- "Coefficient"

lasso_df <- lasso_df %>%
  filter(Coefficient != 0)

ggplot(lasso_df, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "LASSO Regression Coefficients (λ = lambda.min)",
       x = "Variable",
       y = "Coefficient Estimate") +
  theme_minimal()


```

## Logistic Regression:

```{r}

propensity_model <- glm(z ~ selfrpt + race + gender + fgen + urban + mindset + test + sch_race + pov + size, family = binomial(), data = mindset_data)

summary(propensity_model)

```

This model estimates the probability of receiving the treatment (Z = 1) given a set of covariates. This is the propensity score, used later in causal inference methods like: Matching, Inverse Probability Weighting (IPW) and Covariate Adjustment

Propensity scores (predicted probabilities from this model) allow us to mimic randomization in observational data.

This helps us adjust for confounding, ensuring fair comparison between treated and untreated students.


```{r}

mindset_data$pscore <- predict(propensity_model, type = "response")

mindset_data$binary_y <- ifelse(mindset_data$y > median(mindset_data$y), 1, 0)

logit_glm <- glm(binary_y ~ z + selfrpt + gender + fgen + urban + mindset + test + sch_race + pov + size, family = binomial(), data = mindset_data)
summary(logit_glm)

```

Demonstrates classification power of the intervention and other predictors.

The significant and positive treatment effect supports your causal findings from OLS, and LASSO

We can also use this for ROC/AUC analysis to assess model performance.

```{r}

roc_curve <- roc(mindset_data$binary_y, predict(logit_glm, type = "response"))
plot(roc_curve)
auc(roc_curve)

```

The ROC analysis shows that the logistic regression model predicts student success above the median with an AUC of 0.7385, indicating fair-to-good discriminative ability. This supports the validity of the treatment effect and the overall predictive strength of our model.


## Subgroup Analysis:

```{r}

ggplot(mindset_data, aes(x = as.factor(z), y = y, fill = as.factor(z))) +
  stat_summary(fun = mean, geom = "bar", position = "dodge") +
  stat_summary(fun.data = mean_se, geom = "errorbar", position = position_dodge(width = 0.9), width = 0.2) +
  facet_wrap(~ gender) +
  labs(title = "Effect of Treatment on Achievement by Gender",
       x = "Treatment (Z)",
       y = "Mean Achievement (Y)",
       fill = "Treatment") +
  theme_minimal()


```

The intervention positively impacts both gender groups by improving achievement scores compared to the control group.

The effect size appears stronger for females, but both groups benefit from the nudge-like intervention.

In males, the treatment seems to "rescue" students from a negative achievement trend.

```{r}

gender_model <- lm(y ~ z * gender, data = mindset_data)

anova(gender_model)

```

The treatment significantly improves achievement scores, regardless of gender.

Gender itself has a significant influence on achievement, meaning achievement levels are different between genders.

However, since the interaction effect is not significant, we conclude that the treatment effect is similar for both genders—it helps both, without a major difference in impact.

```{r}

ggplot(mindset_data, aes(x = as.factor(z), y = y, fill = as.factor(z))) +
  stat_summary(fun = mean, geom = "bar", position = "dodge") +
  stat_summary(fun.data = mean_se, geom = "errorbar", position = position_dodge(width = 0.9), width = 0.2) +
  facet_wrap(~ fgen) +
  labs(title = "Effect of Treatment on Achievement by First-Generation Status",
       x = "Treatment (Z)",
       y = "Mean Achievement (Y)",
       fill = "Treatment") +
  theme_minimal()


```

The intervention positively impacts students that are first generation and students who aren't first generation by improving achievement scores compared to the control group.

The effect size appears stronger for students who aren't first generation, but both groups benefit from the nudge-like intervention.

In students who are first generation, the treatment seems to "rescue" students from a negative achievement trend.   

```{r}

fgen_model <- lm(y ~ z * fgen, data = mindset_data)

anova(fgen_model)

```

The growth mindset intervention is effective, significantly improving achievement overall.

First-generation students have lower achievement scores than their non-first-generation peers.

The intervention works differently for first-generation students than for non-first-generation students (The interaction term is statistically significant: p $\lt$ 0.05). meaning the program may be more effective for them.

## Poisson Regression:

```{r}

mindset_data$count_y <- round(mindset_data$y - min(mindset_data$y) + 1)

poisson_model <- glm(count_y ~ z + selfrpt + gender + fgen + mindset + test, 
                     family = poisson(), data = mindset_data)
summary(poisson_model)

exp(coef(poisson_model)["z"])

```

Poisson regression models rate-like or count-based outcomes, and while y wasn't originally a count, rounding and transforming it gave us a sense of whether treatment effects scale multiplicatively, not just additively like in linear models.


Poisson regression confirms a positive treatment effect: treated students have a 13.8$\%$ higher expected outcome count, adjusting for key covariates. The direction and magnitude are consistent with earlier linear models, reinforcing the robustness of the causal effect

## Causal Inference: 

### Propensity Score Matching: 

```{r}

match_model <- matchit(z ~ selfrpt + race + gender + fgen + urban + mindset + 
                      test + sch_race + pov + size,
                      data = mindset_data, method = "nearest", distance = "logit")

summary(match_model)

```

```{r}

love.plot(match_model, threshold = 0.1)

```

Red points (Unadjusted): Standardized mean differences before matching.

Blue points (Adjusted): Standardized mean differences after matching.

Dashed lines at -0.1 and 0.1: Threshold for acceptable balance.

After matching, almost all covariates fall within Dashed lines -> good balance, meaning the treatment and control groups are now comparable on observed covariates.


```{r}

matched_data <- match.data(match_model)
match_effect <- lm(y ~ z, data = matched_data)
summary(match_effect)

```

After matching on covariates, students who received the intervention scored on average 0.419 points higher than their matched control peers.

This effect is highly statistically significant and of similar magnitude to your previous models (OLS, LASSO, etc.)

The matching confirms that the positive treatment effect is not due to pre-existing differences between treated and untreated groups.

### Covariate Balance Assessment: 

```{r}

covariates <- c("selfrpt", "mindset", "test", "pov", "size")

smd_before <- data.frame(covariate = character(), smd = numeric())

for (cov in covariates) {
  treated_mean <- mean(mindset_data[[cov]][mindset_data$z == 1])
  control_mean <- mean(mindset_data[[cov]][mindset_data$z == 0])
  pooled_sd <- sqrt((var(mindset_data[[cov]][mindset_data$z == 1]) + 
                   var(mindset_data[[cov]][mindset_data$z == 0])) / 2)
  
  smd <- (treated_mean - control_mean) / pooled_sd
  smd_before <- rbind(smd_before, data.frame(covariate = cov, smd = abs(smd)))
}

smd_after <- data.frame(covariate = character(), smd = numeric())

for (cov in covariates) {
  treated_mean <- mean(matched_data[[cov]][matched_data$z == 1])
  control_mean <- mean(matched_data[[cov]][matched_data$z == 0])
  pooled_sd <- sqrt((var(matched_data[[cov]][matched_data$z == 1]) + 
                   var(matched_data[[cov]][matched_data$z == 0])) / 2)
  
  smd <- (treated_mean - control_mean) / pooled_sd
  smd_after <- rbind(smd_after, data.frame(covariate = cov, smd = abs(smd)))
}

smd_before$timing <- "Before Matching"
smd_after$timing <- "After Matching"
smd_all <- rbind(smd_before, smd_after)

ggplot(smd_all, aes(x = reorder(covariate, smd), y = smd, fill = timing)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_hline(yintercept = 0.1, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(title = "Covariate Balance: Standardized Mean Differences",
       subtitle = "Values < 0.1 indicate good balance",
       y = "Absolute Standardized Mean Difference", 
       x = "Covariate") +
  theme_minimal()

```

Before matching (blue bars): Several covariates (especially selfrpt and mindset) showed moderate imbalance, which could bias the treatment effect estimate.

After matching (red bars): All covariates are now below the 0.1 threshold, indicating excellent covariate balance.

This confirms that your propensity score matching was successful, and the treatment and control groups are now comparable across key confounders.


### Inverse Probability Weighting: 


```{r}

mindset_data$ipw <- ifelse(mindset_data$z == 1, 
                          1/mindset_data$pscore, 
                          1/(1-mindset_data$pscore))

q99 <- quantile(mindset_data$ipw, 0.99)
mindset_data$ipw_trimmed <- pmin(mindset_data$ipw, q99)

ipw_model <- lm(y ~ z, data = mindset_data, weights = ipw_trimmed)
summary(ipw_model)

coeftest(ipw_model, vcov = vcovHC(ipw_model, type = "HC3"))

```

IPW weights adjust for the unequal probability of receiving the treatment (Z). so treated students with low probability of being treated get higher weight, and vice versa.

Trimming caps the top 1$\%$ of weights to the 99th percentile — improves stability and reduces influence of outliers.

We applied Inverse Probability Weighting (IPW) to account for confounding in treatment assignment by reweighting students based on their propensity scores. After trimming extreme weights, the estimated treatment effect remained statistically significant and consistent with our OLS and matching results (Effect size ≈ 0.42). This reinforces the causal conclusion that the growth mindset intervention improves student achievement.

## Augmented Inverse Probability Weighting: 


```{r}

outcome_model <- function(z, data) {
  if (z == 1) {
    model <- lm(y ~ selfrpt + gender + fgen + mindset + test, data = subset(data, z == 1))
    preds <- predict(model, newdata = data)
  } else {
    model <- lm(y ~ selfrpt + gender + fgen + mindset + test, data = subset(data, z == 0))
    preds <- predict(model, newdata = data)
  }
  return(preds)
}

mindset_data$mu1 <- outcome_model(1, mindset_data)
mindset_data$mu0 <- outcome_model(0, mindset_data)

mindset_data$aipw <- with(mindset_data, {
  (z * (y - mu1) / pscore) + mu1 - 
    ((1 - z) * (y - mu0) / (1 - pscore)) - mu0
})

mean(mindset_data$aipw)

set.seed(954567)
aipw_boot <- function(data, indices) {
  d <- data[indices, ]
  prop_model <- glm(z ~ selfrpt + gender + fgen + mindset + test, 
                   family = binomial(), data = d)
  d$ps <- predict(prop_model, type = "response")
  
  d$mu1 <- outcome_model(1, d)
  d$mu0 <- outcome_model(0, d)
  
  d$aipw_est <- with(d, {
    (z * (y - mu1) / ps) + mu1 - 
      ((1 - z) * (y - mu0) / (1 - ps)) - mu0
  })
  
  return(mean(d$aipw_est))
}

aipw_boot_results <- boot(mindset_data, aipw_boot, R = 500)
boot.ci(aipw_boot_results, type = "perc")


```

The AIPW estimator yielded an average treatment effect of 0.415, with a 95% bootstrap confidence interval of (0.391, 0.449). This confirms a statistically significant and robust effect of the growth mindset intervention. AIPW combines the strengths of both outcome regression and propensity score weighting, offering a doubly robust inference framework.


## Bayesian Analysis: 


```{r}


bayes_model <- bayesglm(y ~ z + selfrpt + gender + fgen + mindset + test,
                       data = mindset_data, 
                       prior.scale = 2.5,
                       prior.df = 7)

summary(bayes_model)

coef_sim <- sim(bayes_model, n.sims = 1000)

treatment_effect <- coef_sim@coef[, "z"]
hist(treatment_effect, breaks = 30, 
     main = "Posterior Distribution of Treatment Effect",
     xlab = "Effect Size")

quantile(treatment_effect, c(0.025, 0.975))

```

Bayesian regression yielded a posterior mean treatment effect of 0.417 with a 95% credible interval of (0.391, 0.444). This confirms with high probability that the intervention improves student achievement. The Bayesian approach allows us to quantify uncertainty directly over the effect size, producing a full posterior distribution of estimates. This adds to the robustness and interpretability of our conclusions.




